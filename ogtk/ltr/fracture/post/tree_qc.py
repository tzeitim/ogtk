"""
Phylogenetic Tree QC via Parsimony Scores.

Evaluates tree quality by comparing observed per-column parsimony scores
against a null distribution generated by shuffling the character matrix
while keeping tree topology fixed.
"""

from dataclasses import dataclass
from pathlib import Path

import networkx as nx
import numpy as np
import polars as pl
from scipy import stats


@dataclass
class ColumnQCResult:
    """QC results for a single character column."""
    column: str
    observed_parsimony: int
    null_mean: float
    null_std: float
    z_score: float
    p_value: float


@dataclass
class TreeQCResult:
    """Aggregate QC results for a phylogenetic tree."""
    tree_id: str
    n_leaves: int
    n_columns_analyzed: int
    columns_skipped: list[str]
    columns: list[ColumnQCResult]
    mean_z_score: float
    t_statistic: float
    t_pvalue: float
    quality_class: str  # "good" | "moderate" | "poor"


def compute_parsimony_score(
    tree: nx.DiGraph,
    leaf_states: dict[str, int],
    missing: int = -1,
) -> int:
    """
    Compute parsimony score for one character column using Fitch-Hartigan.

    The algorithm does a bottom-up pass through the tree. At each internal node,
    it computes the intersection of child state sets. If the intersection is empty,
    the union is used and the mutation counter is incremented.

    Parameters
    ----------
    tree : nx.DiGraph
        Directed graph representing the tree (edges point from parent to child).
    leaf_states : dict[str, int]
        Mapping from leaf node names to their character states.
    missing : int
        Value representing missing data (excluded from analysis).

    Returns
    -------
    int
        Parsimony score (number of mutations/unions).
    """
    # Find root (node with in_degree == 0)
    root = None
    for node in tree.nodes():
        if tree.in_degree(node) == 0:
            root = node
            break

    if root is None:
        raise ValueError("Could not find root node (in_degree == 0)")

    # Track mutations during bottom-up traversal
    mutations = 0

    # Post-order traversal using DFS
    def postorder_fitch(node: str) -> set[int]:
        nonlocal mutations

        children = list(tree.successors(node))

        if not children:
            # Leaf node
            state = leaf_states.get(node)
            if state is None or state == missing:
                # Missing data: use universal set (won't cause mutations)
                return set()
            return {state}

        # Internal node: combine children
        child_sets = []
        for child in children:
            child_set = postorder_fitch(child)
            if child_set:  # Only include non-empty sets (missing data)
                child_sets.append(child_set)

        if not child_sets:
            # All children have missing data
            return set()

        # Start with first child's set
        result = child_sets[0].copy()

        for child_set in child_sets[1:]:
            intersection = result & child_set
            if intersection:
                result = intersection
            else:
                # Union required - this is a mutation
                result = result | child_set
                mutations += 1

        return result

    postorder_fitch(root)
    return mutations


def generate_null_distribution(
    tree: nx.DiGraph,
    character_matrix: pl.DataFrame,
    column: str,
    n_permutations: int = 1000,
    seed: int | None = None,
) -> np.ndarray:
    """
    Generate null distribution by shuffling leaf-state assignments.

    The tree topology is kept fixed. The row indices of the character matrix
    are permuted to break the association between leaves and their states.

    Parameters
    ----------
    tree : nx.DiGraph
        Tree topology.
    character_matrix : pl.DataFrame
        Character matrix with 'index' column (leaf names) and state columns.
    column : str
        Which column to permute and score.
    n_permutations : int
        Number of permutations for null distribution.
    seed : int, optional
        Random seed for reproducibility.

    Returns
    -------
    np.ndarray
        Array of parsimony scores from permuted data.
    """
    rng = np.random.default_rng(seed)

    # Get leaf names and states
    leaf_names = character_matrix.get_column('index').to_list()
    states = character_matrix.get_column(column).to_numpy()

    null_scores = np.zeros(n_permutations, dtype=np.int32)

    for i in range(n_permutations):
        # Shuffle states
        shuffled_states = rng.permutation(states)

        # Create new leaf_states mapping
        leaf_states = dict(zip(leaf_names, shuffled_states))

        null_scores[i] = compute_parsimony_score(tree, leaf_states)

    return null_scores


def compute_qc_stats(observed: int, null: np.ndarray) -> dict:
    """
    Compute z-score and p-value comparing observed to null distribution.

    Parameters
    ----------
    observed : int
        Observed parsimony score.
    null : np.ndarray
        Null distribution of parsimony scores.

    Returns
    -------
    dict
        Dictionary with 'null_mean', 'null_std', 'z_score', 'p_value'.
    """
    null_mean = float(np.mean(null))
    null_std = float(np.std(null, ddof=1))

    # Handle zero std (constant null distribution)
    if null_std < 1e-10:
        z_score = 0.0 if observed == null_mean else float('-inf')
    else:
        z_score = (observed - null_mean) / null_std

    # P-value with +1 smoothing
    # Lower observed is better, so we count how often null is <= observed
    n = len(null)
    p_value = (np.sum(null <= observed) + 1) / (n + 1)

    return {
        'null_mean': null_mean,
        'null_std': null_std,
        'z_score': z_score,
        'p_value': p_value,
    }


def tree_parsimony_qc(
    tree_dir: Path,
    n_permutations: int = 1000,
    columns: list[str] | None = None,
    seed: int | None = None,
    tree_key: str = 'nj',
    output_dir: Path | None = None,
) -> TreeQCResult:
    """
    QC a single tree by comparing observed parsimony to null distribution.

    Parameters
    ----------
    tree_dir : Path
        Directory containing tdata.h5td and character_matrix.parquet.
    n_permutations : int
        Number of permutations for null distribution.
    columns : list[str], optional
        Columns to analyze. Defaults to r1-r15.
    seed : int, optional
        Random seed for reproducibility.
    tree_key : str
        Key for tree in tdata.obst (default 'nj').
    output_dir : Path, optional
        If provided, saves results to {output_dir}/{tree_id}.summary.parquet
        and {output_dir}/{tree_id}.columns.parquet.

    Returns
    -------
    TreeQCResult
        QC results including per-column and aggregate statistics.
    """
    import treedata as td

    tree_dir = Path(tree_dir)
    tree_id = tree_dir.name

    # Load tree data
    tdata = td.read_h5td(tree_dir / 'tdata.h5td')
    tree = tdata.obst[tree_key]

    # Load character matrix
    cm = pl.read_parquet(tree_dir / 'character_matrix.parquet')

    # Default columns
    if columns is None:
        columns = [f'r{i}' for i in range(1, 16)]

    # Filter to columns that exist
    available_cols = set(cm.columns)
    columns = [c for c in columns if c in available_cols]

    # Get leaf count
    n_leaves = sum(1 for n in tree.nodes() if tree.out_degree(n) == 0)

    column_results: list[ColumnQCResult] = []
    columns_skipped: list[str] = []

    for col in columns:
        states = cm.get_column(col).to_numpy()
        unique_states = np.unique(states[states >= 0])  # Exclude missing (-1)

        # Skip columns with no variation
        if len(unique_states) <= 1:
            columns_skipped.append(col)
            continue

        # Build leaf states mapping
        leaf_names = cm.get_column('index').to_list()
        leaf_states = dict(zip(leaf_names, states))

        # Compute observed parsimony
        observed = compute_parsimony_score(tree, leaf_states)

        # Generate null distribution
        null = generate_null_distribution(
            tree, cm, col, n_permutations=n_permutations, seed=seed
        )

        # Compute statistics
        qc_stats = compute_qc_stats(observed, null)

        column_results.append(ColumnQCResult(
            column=col,
            observed_parsimony=observed,
            null_mean=qc_stats['null_mean'],
            null_std=qc_stats['null_std'],
            z_score=qc_stats['z_score'],
            p_value=qc_stats['p_value'],
        ))

    # Aggregate statistics
    if column_results:
        z_scores = np.array([r.z_score for r in column_results])
        mean_z_score = float(np.mean(z_scores))

        # One-sample t-test: test if mean z-score differs from 0
        # Need at least 2 samples for t-test
        if len(z_scores) >= 2:
            stat, pval = stats.ttest_1samp(z_scores, 0)
            t_statistic = float(stat)
            t_pvalue = float(pval)
        else:
            # With only 1 column, can't do t-test
            t_statistic = float('nan')
            t_pvalue = float('nan')
    else:
        mean_z_score = 0.0
        t_statistic = 0.0
        t_pvalue = 1.0

    # Quality classification
    if mean_z_score < -2:
        quality_class = "good"
    elif mean_z_score < -1:
        quality_class = "moderate"
    else:
        quality_class = "poor"

    result = TreeQCResult(
        tree_id=tree_id,
        n_leaves=n_leaves,
        n_columns_analyzed=len(column_results),
        columns_skipped=columns_skipped,
        columns=column_results,
        mean_z_score=mean_z_score,
        t_statistic=t_statistic,
        t_pvalue=t_pvalue,
        quality_class=quality_class,
    )

    # Auto-save if output_dir provided
    if output_dir is not None:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        save_qc_result(result, output_dir / tree_id)

    return result


def batch_tree_qc(
    tree_outputs_dir: Path,
    output_dir: Path | None = None,
    n_permutations: int = 1000,
    columns: list[str] | None = None,
    seed: int | None = None,
    tree_key: str = 'nj',
    use_lsf: bool = False,
    lsf_queue: str = 'short',
    lsf_memory: str = '16GB',
    conda_env: str = 'cass11',
    wait: bool = False,
) -> pl.DataFrame | list[str]:
    """
    QC all trees in a directory hierarchy.

    Scans for directories containing both tdata.h5td and character_matrix.parquet.

    Parameters
    ----------
    tree_outputs_dir : Path
        Root directory to scan for tree outputs.
    output_dir : Path, optional
        Directory to save results. Required if use_lsf=True.
    n_permutations : int
        Number of permutations per column.
    columns : list[str], optional
        Columns to analyze.
    seed : int, optional
        Random seed.
    tree_key : str
        Key for tree in tdata.obst.
    use_lsf : bool
        If True, submit jobs via bsub instead of running locally.
    lsf_queue : str
        LSF queue name (default 'short').
    lsf_memory : str
        Memory per job (default '16GB').
    conda_env : str
        Conda/mamba environment to activate (default 'cass11').
    wait : bool
        If True and use_lsf=True, wait for all jobs to complete and return results.

    Returns
    -------
    pl.DataFrame | list[str]
        If use_lsf=False or wait=True: Summary DataFrame with one row per tree.
        If use_lsf=True and wait=False: List of submitted job IDs.
    """
    import subprocess

    tree_outputs_dir = Path(tree_outputs_dir)
    tree_dirs = get_tree_dirs(tree_outputs_dir)

    if not tree_dirs:
        return pl.DataFrame({
            'tree_id': [],
            'tree_path': [],
            'n_leaves': [],
            'n_columns_analyzed': [],
            'n_columns_skipped': [],
            'mean_z_score': [],
            't_statistic': [],
            't_pvalue': [],
            'quality_class': [],
        })

    if use_lsf:
        if output_dir is None:
            raise ValueError("output_dir is required when use_lsf=True")

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        log_dir = output_dir / 'logs'
        log_dir.mkdir(exist_ok=True)

        # Build column args
        col_args = ''
        if columns is not None:
            col_args = f"columns=[{','.join(repr(c) for c in columns)}],"

        job_ids = []
        for tree_dir in tree_dirs:
            tree_id = tree_dir.name
            # Escape path for use in shell
            tree_dir_escaped = str(tree_dir).replace("'", "'\\''")
            output_dir_escaped = str(output_dir).replace("'", "'\\''")

            # Build job script (heredoc-style, passed via stdin)
            job_script = f'''#!/bin/bash
eval "$(/home/projects/nyosef/pedro/miniforge3/bin/conda shell.bash hook)"
source ~/miniforge3/etc/profile.d/mamba.sh
conda activate {conda_env}

python << 'PYTHON_EOF'
from ogtk.ltr.fracture.post.tree_qc import tree_parsimony_qc
from pathlib import Path
tree_parsimony_qc(
    Path('{tree_dir_escaped}'),
    n_permutations={n_permutations},
    {col_args}
    seed={seed},
    tree_key='{tree_key}',
    output_dir=Path('{output_dir_escaped}'),
)
PYTHON_EOF
'''

            bsub_cmd = [
                'bsub',
                '-q', lsf_queue,
                '-J', f'tree_qc_{tree_id[:30]}',
                '-n', '2',
                '-R', 'span[hosts=1]',
                '-R', f'rusage[mem={lsf_memory}]',
                '-o', '/home/projects/nyosef/pedro/log/lsf/%J_tree_qc.out',
                '-e', '/home/projects/nyosef/pedro/log/lsf/%J_tree_qc.err',
            ]

            result = subprocess.run(bsub_cmd, input=job_script, capture_output=True, text=True)
            if result.returncode == 0:
                # Extract job ID from "Job <12345> is submitted..."
                job_id = result.stdout.split('<')[1].split('>')[0]
                job_ids.append(job_id)
            else:
                print(f"Failed to submit {tree_id}: {result.stderr}")

        print(f"Submitted {len(job_ids)} jobs to LSF queue '{lsf_queue}'")
        print(f"Results will be saved to: {output_dir}")
        print(f"Logs: {log_dir}")

        if wait:
            print("Waiting for jobs to complete...")
            _wait_for_lsf_jobs(job_ids)
            summary_df, _ = collect_qc_results(output_dir)
            return summary_df
        else:
            print(f"Run collect_qc_results('{output_dir}') after jobs complete.")
            return job_ids

    # Local execution
    results = []
    for tree_dir in tree_dirs:
        try:
            qc_result = tree_parsimony_qc(
                tree_dir,
                n_permutations=n_permutations,
                columns=columns,
                seed=seed,
                tree_key=tree_key,
                output_dir=output_dir,
            )

            results.append({
                'tree_id': qc_result.tree_id,
                'tree_path': str(tree_dir),
                'n_leaves': qc_result.n_leaves,
                'n_columns_analyzed': qc_result.n_columns_analyzed,
                'n_columns_skipped': len(qc_result.columns_skipped),
                'mean_z_score': qc_result.mean_z_score,
                't_statistic': qc_result.t_statistic,
                't_pvalue': qc_result.t_pvalue,
                'quality_class': qc_result.quality_class,
            })
        except Exception as e:
            results.append({
                'tree_id': tree_dir.name,
                'tree_path': str(tree_dir),
                'n_leaves': None,
                'n_columns_analyzed': None,
                'n_columns_skipped': None,
                'mean_z_score': None,
                't_statistic': None,
                't_pvalue': None,
                'quality_class': f"error: {str(e)[:50]}",
            })

    return pl.DataFrame(results)


def _wait_for_lsf_jobs(job_ids: list[str], poll_interval: int = 30) -> None:
    """Wait for LSF jobs to complete."""
    import subprocess
    import time

    pending = set(job_ids)
    while pending:
        time.sleep(poll_interval)
        # Check which jobs are still running
        result = subprocess.run(
            ['bjobs', '-noheader'] + list(pending),
            capture_output=True, text=True
        )
        still_running = set()
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split()
                if parts and parts[0] in pending:
                    still_running.add(parts[0])
        pending = still_running
        if pending:
            print(f"  {len(pending)} jobs still running...")


def qc_result_to_dataframe(result: TreeQCResult) -> pl.DataFrame:
    """
    Convert a TreeQCResult to a detailed DataFrame with per-column results.

    Parameters
    ----------
    result : TreeQCResult
        QC result from tree_parsimony_qc.

    Returns
    -------
    pl.DataFrame
        DataFrame with one row per column analyzed.
    """
    if not result.columns:
        return pl.DataFrame({
            'tree_id': [],
            'column': [],
            'observed_parsimony': [],
            'null_mean': [],
            'null_std': [],
            'z_score': [],
            'p_value': [],
        })

    return pl.DataFrame([
        {
            'tree_id': result.tree_id,
            'column': col.column,
            'observed_parsimony': col.observed_parsimony,
            'null_mean': col.null_mean,
            'null_std': col.null_std,
            'z_score': col.z_score,
            'p_value': col.p_value,
        }
        for col in result.columns
    ])


def get_tree_dirs(tree_outputs_dir: Path) -> list[Path]:
    """
    Find all valid tree directories for QC.

    Scans for directories containing both tdata.h5td and character_matrix.parquet.

    Parameters
    ----------
    tree_outputs_dir : Path
        Root directory to scan.

    Returns
    -------
    list[Path]
        List of tree directories ready for QC.
    """
    tree_outputs_dir = Path(tree_outputs_dir)
    tree_dirs = []

    for tdata_path in tree_outputs_dir.rglob('tdata.h5td'):
        tree_dir = tdata_path.parent
        cm_path = tree_dir / 'character_matrix.parquet'
        if cm_path.exists():
            tree_dirs.append(tree_dir)

    return sorted(tree_dirs)


def save_qc_result(result: TreeQCResult, output_path: Path) -> None:
    """
    Save a TreeQCResult to parquet files.

    Creates two files:
    - {output_path}_summary.parquet: aggregate stats
    - {output_path}_columns.parquet: per-column details

    Parameters
    ----------
    result : TreeQCResult
        QC result from tree_parsimony_qc.
    output_path : Path
        Base path for output files (without extension).
    """
    output_path = Path(output_path)

    # Summary
    summary = pl.DataFrame([{
        'tree_id': result.tree_id,
        'n_leaves': result.n_leaves,
        'n_columns_analyzed': result.n_columns_analyzed,
        'n_columns_skipped': len(result.columns_skipped),
        'columns_skipped': ','.join(result.columns_skipped),
        'mean_z_score': result.mean_z_score,
        't_statistic': result.t_statistic,
        't_pvalue': result.t_pvalue,
        'quality_class': result.quality_class,
    }])
    summary.write_parquet(output_path.with_suffix('.summary.parquet'))

    # Per-column details
    columns_df = qc_result_to_dataframe(result)
    columns_df.write_parquet(output_path.with_suffix('.columns.parquet'))


def collect_qc_results(qc_output_dir: Path) -> tuple[pl.DataFrame, pl.DataFrame]:
    """
    Collect QC results from multiple saved files.

    Parameters
    ----------
    qc_output_dir : Path
        Directory containing *.summary.parquet and *.columns.parquet files.

    Returns
    -------
    tuple[pl.DataFrame, pl.DataFrame]
        (summary_df, columns_df) - aggregated results from all files.
    """
    qc_output_dir = Path(qc_output_dir)

    summary_files = list(qc_output_dir.glob('*.summary.parquet'))
    columns_files = list(qc_output_dir.glob('*.columns.parquet'))

    if summary_files:
        summary_df = pl.concat([pl.read_parquet(f) for f in summary_files])
    else:
        summary_df = pl.DataFrame()

    if columns_files:
        columns_df = pl.concat([pl.read_parquet(f) for f in columns_files])
    else:
        columns_df = pl.DataFrame()

    return summary_df, columns_df
